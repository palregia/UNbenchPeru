{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('./task2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columnas en el CSV: ['Country', 'Original_id', 'Authors', 'Voting', 'Date']\n",
      "\n",
      "Primeras filas del DataFrame:\n",
      "          Country  Original_id  \\\n",
      "0  United Kingdom           72   \n",
      "1          France           73   \n",
      "2          Russia          322   \n",
      "3   United States          341   \n",
      "4   United States          455   \n",
      "\n",
      "                                             Authors Voting        Date  \n",
      "0  ['Bolivia (Plurinational State of)', 'China', ...      Y  2018-01-30  \n",
      "1  ['Bolivia (Plurinational State of)', 'China', ...      Y  2018-01-30  \n",
      "2                                                NaN      Y   13/4/2018  \n",
      "3  ['Bolivia (Plurinational State of)', 'China', ...      Y   23/4/2018  \n",
      "4  ['Bolivia (Plurinational State of)', 'China', ...      Y   15/5/2018  \n",
      "\n",
      "Archivos JSON encontrados: 10\n",
      "✓ Procesado: S_2018_1147-EN.json\n",
      "✓ Procesado: S_2018_455-EN.json\n",
      "✓ Procesado: S_2018_72-EN.json\n",
      "✓ Procesado: S_2018_1110-EN.json\n",
      "✓ Procesado: S_2018_1109-EN.json\n",
      "✓ Procesado: S_2018_73-EN.json\n",
      "✓ Procesado: S_2018_341-EN.json\n",
      "✓ Procesado: S_2018_322-EN.json\n",
      "✓ Procesado: S_2018_1016-EN.json\n",
      "✓ Procesado: S_2018_1143-EN.json\n",
      "\n",
      "Total de documentos procesados: 10\n",
      "\n",
      "Ejemplo del primer documento (primeros 200 caracteres):\n",
      " United Nations   S/2018/1147  \n",
      "  Security  Council   Distr.: General  \n",
      "21 December 2018  \n",
      "Original: English  \n",
      " \n",
      "18-22561 (E)    241218  \n",
      "*1822561*   \n",
      " \n",
      "  United Kingdom of Great Britain and Northern \n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "\n",
    "# Cargar el CSV\n",
    "df = pd.read_csv('./task2.csv')\n",
    "\n",
    "# Primero, veamos qué hay en el DataFrame\n",
    "print(\"Columnas en el CSV:\", df.columns.tolist())\n",
    "print(\"\\nPrimeras filas del DataFrame:\")\n",
    "print(df.head())\n",
    "\n",
    "# Extraer los IDs únicos de la carpeta 2018\n",
    "path = './task2/2018'\n",
    "json_files = [f for f in os.listdir(path) if f.endswith('EN.json')]\n",
    "print(f\"\\nArchivos JSON encontrados: {len(json_files)}\")\n",
    "\n",
    "drafts = []\n",
    "for json_file in json_files:\n",
    "    try:\n",
    "        with open(os.path.join(path, json_file)) as f:\n",
    "            data = json.load(f)\n",
    "            drafts.append(data['Content'])\n",
    "            print(f\"✓ Procesado: {json_file}\")\n",
    "    except Exception as e:\n",
    "        print(f\"✗ Error procesando {json_file}: {str(e)}\")\n",
    "\n",
    "print(f\"\\nTotal de documentos procesados: {len(drafts)}\")\n",
    "\n",
    "# Si se procesaron documentos, mostrar ejemplo del contenido\n",
    "if drafts:\n",
    "    print(\"\\nEjemplo del primer documento (primeros 200 caracteres):\")\n",
    "    print(drafts[0][:200])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If use Together API\n",
    "from together import Together\n",
    "\n",
    "your_model_name = 'meta-llama/Llama-3.3-70B-Instruct-Turbo-Free'\n",
    "your_api_key = '574bf95f6b33074e60f284c2c6316e72c5ef8226ce8ce4650dacca4d51ce1050'\n",
    "client = Together(api_key=your_api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columnas en el CSV: ['Country', 'Original_id', 'Authors', 'Voting', 'Date']\n",
      "\n",
      "Primeras filas del DataFrame:\n",
      "          Country  Original_id  \\\n",
      "0  United Kingdom           72   \n",
      "1          France           73   \n",
      "2          Russia          322   \n",
      "3   United States          341   \n",
      "4   United States          455   \n",
      "\n",
      "                                             Authors Voting        Date  \n",
      "0  ['Bolivia (Plurinational State of)', 'China', ...      Y  2018-01-30  \n",
      "1  ['Bolivia (Plurinational State of)', 'China', ...      Y  2018-01-30  \n",
      "2                                                NaN      Y   13/4/2018  \n",
      "3  ['Bolivia (Plurinational State of)', 'China', ...      Y   23/4/2018  \n",
      "4  ['Bolivia (Plurinational State of)', 'China', ...      Y   15/5/2018  \n",
      "\n",
      "Archivos JSON encontrados: 10\n",
      "✓ Procesado: S_2018_1147-EN.json\n",
      "✓ Procesado: S_2018_455-EN.json\n",
      "✓ Procesado: S_2018_72-EN.json\n",
      "✓ Procesado: S_2018_1110-EN.json\n",
      "✓ Procesado: S_2018_1109-EN.json\n",
      "✓ Procesado: S_2018_73-EN.json\n",
      "✓ Procesado: S_2018_341-EN.json\n",
      "✓ Procesado: S_2018_322-EN.json\n",
      "✓ Procesado: S_2018_1016-EN.json\n",
      "✓ Procesado: S_2018_1143-EN.json\n",
      "\n",
      "Total de documentos procesados: 10\n",
      "\n",
      "Procesando documentos con el modelo...\n",
      "País: United Kingdom, Voto: Y\n",
      "País: France, Voto: Y\n",
      "País: Russia, Voto: N\n",
      "País: United States, Voto: Y\n",
      "País: United States, Voto: Y\n",
      "País: France, Voto: Y\n",
      "\n",
      "Respuesta inválida para Côte d’Ivoire, Equatorial Guinea, Ethiopia, France, Netherlands,\n",
      "Peru, Poland, Sweden, United Kingdom and United States: As\n",
      "País: Côte d’Ivoire, Equatorial Guinea, Ethiopia, France, Netherlands,\n",
      "Peru, Poland, Sweden, United Kingdom and United States, Voto: Y\n",
      "País: Kuwait and Sweden, Voto: Y\n",
      "\n",
      "Respuesta inválida para Russian and United States: As\n",
      "País: Russian and United States, Voto: N\n",
      "País: United Kingdom, Voto: Y\n",
      "\n",
      "Ejemplo del primer documento (primeros 200 caracteres):\n",
      " United Nations   S/2018/1147  \n",
      "  Security  Council   Distr.: General  \n",
      "21 December 2018  \n",
      "Original: English  \n",
      " \n",
      "18-22561 (E)    241218  \n",
      "*1822561*   \n",
      " \n",
      "  United Kingdom of Great Britain and Northern \n",
      "\n",
      "Índices con respuestas inválidas: [6, 8]\n",
      "Accuracy: 0.8\n",
      "AUC: nan\n",
      "Balanced Accuracy: 0.8\n",
      "Precision: 0.5\n",
      "Recall: 0.4\n",
      "F1: 0.4444444444444444\n",
      "PR AUC: 1.0\n",
      "MCC: 0.0\n",
      "G-Mean: 0.4\n",
      "Accuracy AUC Balanced_Acc Precision Recall F1 PR_AUC MCC G-Mean\n",
      "0.8000 nan 0.8000 0.5000 0.4000 0.4444 1.0000 0.0000 0.4000\n",
      "\n",
      "Índices con respuestas inválidas: [6, 8]\n",
      "Accuracy: 0.8\n",
      "AUC: nan\n",
      "Balanced Accuracy: 0.8\n",
      "Precision: 0.5\n",
      "Recall: 0.4\n",
      "F1: 0.4444444444444444\n",
      "PR AUC: 1.0\n",
      "MCC: 0.0\n",
      "G-Mean: 0.4\n",
      "Accuracy AUC Balanced_Acc Precision Recall F1 PR_AUC MCC G-Mean\n",
      "0.8000 nan 0.8000 0.5000 0.4000 0.4444 1.0000 0.0000 0.4000\n",
      "\n",
      "Precisión: 80.00%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/deca3/UNBench/venv/lib/python3.9/site-packages/sklearn/metrics/_ranking.py:379: UndefinedMetricWarning: Only one class is present in y_true. ROC AUC score is not defined in that case.\n",
      "  warnings.warn(\n",
      "/Users/deca3/UNBench/venv/lib/python3.9/site-packages/sklearn/metrics/_classification.py:2524: UserWarning: y_pred contains classes not in y_true\n",
      "  warnings.warn(\"y_pred contains classes not in y_true\")\n",
      "/Users/deca3/UNBench/venv/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/Users/deca3/UNBench/venv/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Specificity is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/Users/deca3/UNBench/venv/lib/python3.9/site-packages/sklearn/metrics/_ranking.py:379: UndefinedMetricWarning: Only one class is present in y_true. ROC AUC score is not defined in that case.\n",
      "  warnings.warn(\n",
      "/Users/deca3/UNBench/venv/lib/python3.9/site-packages/sklearn/metrics/_classification.py:2524: UserWarning: y_pred contains classes not in y_true\n",
      "  warnings.warn(\"y_pred contains classes not in y_true\")\n",
      "/Users/deca3/UNBench/venv/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/Users/deca3/UNBench/venv/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Specificity is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import os\n",
    "import json\n",
    "from together import Together\n",
    "from sklearn.metrics import accuracy_score, balanced_accuracy_score, precision_recall_fscore_support\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score, matthews_corrcoef\n",
    "from sklearn.preprocessing import LabelEncoder, label_binarize\n",
    "from imblearn.metrics import geometric_mean_score\n",
    "import numpy as np\n",
    "\n",
    "# Definir la función calculate_metrics primero\n",
    "def calculate_metrics(pred, labels):\n",
    "    label_encoder = LabelEncoder()\n",
    "    all_classes = list(set(labels) | set(pred))\n",
    "    label_encoder.fit(all_classes)\n",
    "\n",
    "    labels = label_encoder.transform(labels)\n",
    "    pred = label_encoder.transform(pred)\n",
    "\n",
    "    acc = accuracy_score(labels, pred)\n",
    "    \n",
    "    num_classes = len(label_encoder.classes_)\n",
    "    true_labels_bin = label_binarize(labels, classes=list(range(num_classes)))\n",
    "    pred_bin = label_binarize(pred, classes=list(range(num_classes)))\n",
    "\n",
    "    auc = roc_auc_score(true_labels_bin, pred_bin, multi_class='ovr', average='macro')\n",
    "    pr_auc = average_precision_score(true_labels_bin, pred_bin, average='macro')\n",
    "\n",
    "    balanced_acc = balanced_accuracy_score(labels, pred)\n",
    "    prec, rec, f1, _ = precision_recall_fscore_support(labels, pred, average='macro')\n",
    "\n",
    "    mcc = matthews_corrcoef(labels, pred)\n",
    "    g_mean = geometric_mean_score(labels, pred, average='macro')\n",
    "\n",
    "    print(f'Accuracy: {acc}')\n",
    "    print(f'AUC: {auc}')\n",
    "    print(f'Balanced Accuracy: {balanced_acc}')\n",
    "    print(f'Precision: {prec}')\n",
    "    print(f'Recall: {rec}')\n",
    "    print(f'F1: {f1}')\n",
    "    print(f'PR AUC: {pr_auc}')\n",
    "    print(f'MCC: {mcc}')\n",
    "    print(f'G-Mean: {g_mean}')\n",
    "\n",
    "    print('Accuracy AUC Balanced_Acc Precision Recall F1 PR_AUC MCC G-Mean')\n",
    "    print(f'{acc:.4f} {auc:.4f} {balanced_acc:.4f} {prec:.4f} {rec:.4f} {f1:.4f} {pr_auc:.4f} {mcc:.4f} {g_mean:.4f}')\n",
    "\n",
    "# Inicializar todas las variables necesarias\n",
    "drafts = []\n",
    "pred = []\n",
    "invalid_responses = []\n",
    "votes = []\n",
    "\n",
    "# Cargar el CSV y obtener los votos\n",
    "df = pd.read_csv('./task2.csv')\n",
    "votes = list(df['Voting'])\n",
    "\n",
    "# Configurar Together API\n",
    "your_model_name = 'meta-llama/Llama-3.3-70B-Instruct-Turbo-Free'\n",
    "your_api_key = '574bf95f6b33074e60f284c2c6316e72c5ef8226ce8ce4650dacca4d51ce1050'\n",
    "client = Together(api_key=your_api_key)\n",
    "\n",
    "# Mostrar información del DataFrame\n",
    "print(\"Columnas en el CSV:\", df.columns.tolist())\n",
    "print(\"\\nPrimeras filas del DataFrame:\")\n",
    "print(df.head())\n",
    "\n",
    "# Procesar archivos JSON\n",
    "path = './task2/2018'\n",
    "json_files = [f for f in os.listdir(path) if f.endswith('EN.json')]\n",
    "print(f\"\\nArchivos JSON encontrados: {len(json_files)}\")\n",
    "\n",
    "# Cargar contenido de los archivos JSON\n",
    "for json_file in json_files:\n",
    "    try:\n",
    "        with open(os.path.join(path, json_file)) as f:\n",
    "            data = json.load(f)\n",
    "            drafts.append(data['Content'])\n",
    "            print(f\"✓ Procesado: {json_file}\")\n",
    "    except Exception as e:\n",
    "        print(f\"✗ Error procesando {json_file}: {str(e)}\")\n",
    "\n",
    "print(f\"\\nTotal de documentos procesados: {len(drafts)}\")\n",
    "\n",
    "def truncate_text(text, max_chars=2000):\n",
    "    if len(text) > max_chars:\n",
    "        return text[:max_chars] + \"... [Texto truncado]\"\n",
    "    return text\n",
    "\n",
    "# Procesar documentos con el modelo\n",
    "print(\"\\nProcesando documentos con el modelo...\")\n",
    "for i, draft in enumerate(drafts):\n",
    "    try:\n",
    "        truncated_draft = truncate_text(draft)\n",
    "        country = df.iloc[i]['Country'] if i < len(df) else \"Unknown\"\n",
    "        \n",
    "        system_prompt = \"You are a UN diplomat. Vote Y (Yes), N (No), or A (Abstain).\"\n",
    "        user_prompt = f\"\"\"As {country}'s diplomat, vote on this UN resolution:\n",
    "\n",
    "{truncated_draft}\n",
    "\n",
    "Vote (Y/N/A):\"\"\"\n",
    "        \n",
    "        response = client.chat.completions.create(\n",
    "            model=your_model_name,\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": system_prompt},\n",
    "                {\"role\": \"user\", \"content\": user_prompt}\n",
    "            ],\n",
    "            max_tokens=1,\n",
    "            temperature=0.0\n",
    "        )\n",
    "        \n",
    "        result = response.choices[0].message.content.strip()\n",
    "        valid_votes = ['Y', 'N', 'A']\n",
    "        \n",
    "        if result not in valid_votes:\n",
    "            print(f\"\\nRespuesta inválida para {country}: {result}\")\n",
    "            result = random.choice(valid_votes)\n",
    "            invalid_responses.append(i)\n",
    "        \n",
    "        pred.append(result)\n",
    "        print(f\"País: {country}, Voto: {result}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"\\nError procesando documento {i}: {str(e)}\")\n",
    "        result = random.choice(['Y', 'N', 'A'])\n",
    "        invalid_responses.append(i)\n",
    "        pred.append(result)\n",
    "\n",
    "# Mostrar ejemplo del primer documento\n",
    "if drafts:\n",
    "    print(\"\\nEjemplo del primer documento (primeros 200 caracteres):\")\n",
    "    print(drafts[0][:200])\n",
    "\n",
    "# Mostrar respuestas inválidas\n",
    "if len(invalid_responses) > 0:\n",
    "    print(\"\\nÍndices con respuestas inválidas:\", invalid_responses)\n",
    "\n",
    "# Calcular métricas usando la función ya definida\n",
    "if len(pred) > 0 and len(votes) > 0:\n",
    "    calculate_metrics(pred, votes)\n",
    "\n",
    "# Mostrar respuestas inválidas\n",
    "if len(invalid_responses) > 0:\n",
    "    print(\"\\nÍndices con respuestas inválidas:\", invalid_responses)\n",
    "\n",
    "# Calcular métricas\n",
    "if len(pred) > 0 and len(votes) > 0:\n",
    "    calculate_metrics(pred, votes)\n",
    "# Si contamos con los votos reales, calculamos la precisión\n",
    "if 'votes' in locals():\n",
    "    correct = sum(1 for p, v in zip(pred, votes) if p == v)\n",
    "    accuracy = correct / len(votes)\n",
    "    print(f\"\\nPrecisión: {accuracy:.2%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate metrics\n",
    "from sklearn.metrics import accuracy_score, balanced_accuracy_score, precision_recall_fscore_support\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score, matthews_corrcoef\n",
    "from sklearn.preprocessing import LabelEncoder, label_binarize\n",
    "from imblearn.metrics import geometric_mean_score\n",
    "import numpy as np\n",
    "\n",
    "# calculate metrics\n",
    "from sklearn.metrics import accuracy_score, balanced_accuracy_score, precision_recall_fscore_support\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score, matthews_corrcoef\n",
    "from sklearn.preprocessing import LabelEncoder, label_binarize\n",
    "from imblearn.metrics import geometric_mean_score\n",
    "import numpy as np\n",
    "import warnings\n",
    "\n",
    "def calculate_metrics(pred, labels):\n",
    "    # Suprimir todos los warnings\n",
    "    warnings.filterwarnings('ignore')\n",
    "    \n",
    "    # Convertir a arrays numpy\n",
    "    pred = np.array(pred)\n",
    "    labels = np.array(labels)\n",
    "    \n",
    "    # Obtener clases únicas presentes en ambos conjuntos\n",
    "    unique_classes = np.unique(np.concatenate([pred, labels]))\n",
    "    \n",
    "    # Métricas básicas\n",
    "    acc = accuracy_score(labels, pred)\n",
    "    balanced_acc = balanced_accuracy_score(labels, pred)\n",
    "    \n",
    "    # Calcular precision, recall y F1 con manejo de zero_division\n",
    "    prec, rec, f1, _ = precision_recall_fscore_support(\n",
    "        labels, \n",
    "        pred, \n",
    "        average='macro', \n",
    "        zero_division=1,  # Cambiar a 1 para evitar warnings\n",
    "        labels=unique_classes\n",
    "    )\n",
    "    \n",
    "    # Calcular MCC\n",
    "    mcc = matthews_corrcoef(labels, pred)\n",
    "    \n",
    "    # Calcular G-Mean con manejo de errores\n",
    "    with warnings.catch_warnings():\n",
    "        warnings.simplefilter(\"ignore\")\n",
    "        g_mean = geometric_mean_score(labels, pred, average='macro')\n",
    "\n",
    "    # Imprimir resultados básicos\n",
    "    print(\"\\nMétricas básicas:\")\n",
    "    print(f'Accuracy: {acc:.4f}')\n",
    "    print(f'Balanced Accuracy: {balanced_acc:.4f}')\n",
    "    print(f'Precision: {prec:.4f}')\n",
    "    print(f'Recall: {rec:.4f}')\n",
    "    print(f'F1: {f1:.4f}')\n",
    "    print(f'MCC: {mcc:.4f}')\n",
    "    print(f'G-Mean: {g_mean:.4f}')\n",
    "    \n",
    "    # Calcular AUC y PR-AUC solo si hay múltiples clases\n",
    "    if len(unique_classes) > 1:\n",
    "        try:\n",
    "            # Preparar datos para métricas binarias/multiclase\n",
    "            label_encoder = LabelEncoder()\n",
    "            label_encoder.fit(unique_classes)\n",
    "            y_true_encoded = label_encoder.transform(labels)\n",
    "            y_pred_encoded = label_encoder.transform(pred)\n",
    "            \n",
    "            # Binarizar las etiquetas\n",
    "            y_true_bin = label_binarize(y_true_encoded, classes=range(len(unique_classes)))\n",
    "            y_pred_bin = label_binarize(y_pred_encoded, classes=range(len(unique_classes)))\n",
    "            \n",
    "            # Calcular AUC y PR-AUC\n",
    "            auc = roc_auc_score(y_true_bin, y_pred_bin, multi_class='ovr', average='macro')\n",
    "            pr_auc = average_precision_score(y_true_bin, y_pred_bin, average='macro')\n",
    "            \n",
    "            print('\\nMétricas adicionales:')\n",
    "            print(f'AUC: {auc:.4f}')\n",
    "            print(f'PR AUC: {pr_auc:.4f}')\n",
    "        except Exception as e:\n",
    "            print('\\nMétricas adicionales:')\n",
    "            print('AUC: N/A')\n",
    "            print('PR AUC: N/A')\n",
    "            auc = float('nan')\n",
    "            pr_auc = float('nan')\n",
    "    else:\n",
    "        print('\\nMétricas adicionales:')\n",
    "        print('AUC: N/A (solo una clase presente)')\n",
    "        print('PR AUC: N/A (solo una clase presente)')\n",
    "        auc = float('nan')\n",
    "        pr_auc = float('nan')\n",
    "    \n",
    "    # Imprimir resumen final\n",
    "    print('\\nResumen de métricas:')\n",
    "    print('Accuracy Balanced_Acc Precision Recall F1 MCC G-Mean')\n",
    "    print(f'{acc:.4f} {balanced_acc:.4f} {prec:.4f} {rec:.4f} {f1:.4f} {mcc:.4f} {g_mean:.4f}')\n",
    "\n",
    "    return {\n",
    "        'accuracy': acc,\n",
    "        'balanced_accuracy': balanced_acc,\n",
    "        'precision': prec,\n",
    "        'recall': rec,\n",
    "        'f1': f1,\n",
    "        'mcc': mcc,\n",
    "        'g_mean': g_mean,\n",
    "        'auc': auc,\n",
    "        'pr_auc': pr_auc\n",
    "    }\n",
    "\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Métricas básicas:\n",
      "Accuracy: 0.8000\n",
      "Balanced Accuracy: 0.8000\n",
      "Precision: 0.5000\n",
      "Recall: 0.9000\n",
      "F1: 0.4444\n",
      "MCC: 0.0000\n",
      "G-Mean: 0.4000\n",
      "\n",
      "Métricas adicionales:\n",
      "AUC: nan\n",
      "PR AUC: 1.0000\n",
      "\n",
      "Resumen de métricas:\n",
      "Accuracy Balanced_Acc Precision Recall F1 MCC G-Mean\n",
      "0.8000 0.8000 0.5000 0.9000 0.4444 0.0000 0.4000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'accuracy': 0.8,\n",
       " 'balanced_accuracy': np.float64(0.8),\n",
       " 'precision': 0.5,\n",
       " 'recall': 0.9,\n",
       " 'f1': 0.4444444444444444,\n",
       " 'mcc': 0.0,\n",
       " 'g_mean': np.float64(0.4),\n",
       " 'auc': nan,\n",
       " 'pr_auc': np.float64(1.0)}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calculate_metrics(pred, votes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
